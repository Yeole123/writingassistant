{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "116325ca-f1d9-4caa-867e-2d598acc98c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: simpletransformers==0.60.9 in c:\\users\\athar\\anaconda3\\lib\\site-packages (0.60.9)\n",
      "Requirement already satisfied: numpy in c:\\users\\athar\\anaconda3\\lib\\site-packages (from simpletransformers==0.60.9) (1.26.4)\n",
      "Requirement already satisfied: requests in c:\\users\\athar\\anaconda3\\lib\\site-packages (from simpletransformers==0.60.9) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.47.0 in c:\\users\\athar\\anaconda3\\lib\\site-packages (from simpletransformers==0.60.9) (4.66.4)\n",
      "Requirement already satisfied: regex in c:\\users\\athar\\anaconda3\\lib\\site-packages (from simpletransformers==0.60.9) (2023.10.3)\n",
      "Requirement already satisfied: transformers>=4.2.0 in c:\\users\\athar\\anaconda3\\lib\\site-packages (from simpletransformers==0.60.9) (4.44.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\athar\\anaconda3\\lib\\site-packages (from simpletransformers==0.60.9) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\athar\\anaconda3\\lib\\site-packages (from simpletransformers==0.60.9) (1.4.2)\n",
      "Requirement already satisfied: seqeval in c:\\users\\athar\\anaconda3\\lib\\site-packages (from simpletransformers==0.60.9) (1.2.2)\n",
      "Requirement already satisfied: tensorboardx in c:\\users\\athar\\anaconda3\\lib\\site-packages (from simpletransformers==0.60.9) (2.6.2.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\athar\\anaconda3\\lib\\site-packages (from simpletransformers==0.60.9) (2.2.2)\n",
      "Requirement already satisfied: tokenizers in c:\\users\\athar\\anaconda3\\lib\\site-packages (from simpletransformers==0.60.9) (0.19.1)\n",
      "Requirement already satisfied: wandb in c:\\users\\athar\\anaconda3\\lib\\site-packages (from simpletransformers==0.60.9) (0.17.9)\n",
      "Requirement already satisfied: streamlit in c:\\users\\athar\\anaconda3\\lib\\site-packages (from simpletransformers==0.60.9) (1.32.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\athar\\anaconda3\\lib\\site-packages (from simpletransformers==0.60.9) (0.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\athar\\anaconda3\\lib\\site-packages (from tqdm>=4.47.0->simpletransformers==0.60.9) (0.4.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\athar\\anaconda3\\lib\\site-packages (from transformers>=4.2.0->simpletransformers==0.60.9) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\athar\\anaconda3\\lib\\site-packages (from transformers>=4.2.0->simpletransformers==0.60.9) (0.24.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\athar\\anaconda3\\lib\\site-packages (from transformers>=4.2.0->simpletransformers==0.60.9) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\athar\\anaconda3\\lib\\site-packages (from transformers>=4.2.0->simpletransformers==0.60.9) (6.0.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\athar\\anaconda3\\lib\\site-packages (from transformers>=4.2.0->simpletransformers==0.60.9) (0.4.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\athar\\anaconda3\\lib\\site-packages (from pandas->simpletransformers==0.60.9) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\athar\\anaconda3\\lib\\site-packages (from pandas->simpletransformers==0.60.9) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\athar\\anaconda3\\lib\\site-packages (from pandas->simpletransformers==0.60.9) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\athar\\anaconda3\\lib\\site-packages (from requests->simpletransformers==0.60.9) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\athar\\anaconda3\\lib\\site-packages (from requests->simpletransformers==0.60.9) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\athar\\anaconda3\\lib\\site-packages (from requests->simpletransformers==0.60.9) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\athar\\anaconda3\\lib\\site-packages (from requests->simpletransformers==0.60.9) (2024.8.30)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\athar\\anaconda3\\lib\\site-packages (from scikit-learn->simpletransformers==0.60.9) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\athar\\anaconda3\\lib\\site-packages (from scikit-learn->simpletransformers==0.60.9) (2.2.0)\n",
      "Requirement already satisfied: altair<6,>=4.0 in c:\\users\\athar\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers==0.60.9) (5.0.1)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in c:\\users\\athar\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers==0.60.9) (1.6.2)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in c:\\users\\athar\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers==0.60.9) (5.3.3)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\athar\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers==0.60.9) (8.1.7)\n",
      "Requirement already satisfied: pillow<11,>=7.1.0 in c:\\users\\athar\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers==0.60.9) (10.3.0)\n",
      "Requirement already satisfied: protobuf<5,>=3.20 in c:\\users\\athar\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers==0.60.9) (3.20.3)\n",
      "Requirement already satisfied: pyarrow>=7.0 in c:\\users\\athar\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers==0.60.9) (14.0.2)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in c:\\users\\athar\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers==0.60.9) (13.3.5)\n",
      "Requirement already satisfied: tenacity<9,>=8.1.0 in c:\\users\\athar\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers==0.60.9) (8.2.2)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\users\\athar\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers==0.60.9) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in c:\\users\\athar\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers==0.60.9) (4.11.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\users\\athar\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers==0.60.9) (3.1.37)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in c:\\users\\athar\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers==0.60.9) (0.8.0)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\athar\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers==0.60.9) (6.4.1)\n",
      "Requirement already satisfied: watchdog>=2.1.5 in c:\\users\\athar\\anaconda3\\lib\\site-packages (from streamlit->simpletransformers==0.60.9) (4.0.1)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\athar\\anaconda3\\lib\\site-packages (from wandb->simpletransformers==0.60.9) (0.4.0)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\athar\\anaconda3\\lib\\site-packages (from wandb->simpletransformers==0.60.9) (3.10.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\athar\\anaconda3\\lib\\site-packages (from wandb->simpletransformers==0.60.9) (5.9.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in c:\\users\\athar\\anaconda3\\lib\\site-packages (from wandb->simpletransformers==0.60.9) (2.14.0)\n",
      "Requirement already satisfied: setproctitle in c:\\users\\athar\\anaconda3\\lib\\site-packages (from wandb->simpletransformers==0.60.9) (1.3.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\athar\\anaconda3\\lib\\site-packages (from wandb->simpletransformers==0.60.9) (69.5.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\athar\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit->simpletransformers==0.60.9) (3.1.4)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\athar\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit->simpletransformers==0.60.9) (4.19.2)\n",
      "Requirement already satisfied: toolz in c:\\users\\athar\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit->simpletransformers==0.60.9) (0.12.0)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\athar\\anaconda3\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb->simpletransformers==0.60.9) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\athar\\anaconda3\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit->simpletransformers==0.60.9) (4.0.7)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\athar\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers>=4.2.0->simpletransformers==0.60.9) (2024.3.1)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\athar\\anaconda3\\lib\\site-packages (from rich<14,>=10.14.0->streamlit->simpletransformers==0.60.9) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\athar\\anaconda3\\lib\\site-packages (from rich<14,>=10.14.0->streamlit->simpletransformers==0.60.9) (2.15.1)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in c:\\users\\athar\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit->simpletransformers==0.60.9) (4.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\athar\\anaconda3\\lib\\site-packages (from jinja2->altair<6,>=4.0->streamlit->simpletransformers==0.60.9) (2.1.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\athar\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers==0.60.9) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\athar\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers==0.60.9) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\athar\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers==0.60.9) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\athar\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers==0.60.9) (0.10.6)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\athar\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich<14,>=10.14.0->streamlit->simpletransformers==0.60.9) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install simpletransformers==0.60.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abdb2d75-185a-4e13-ba98-4d2a2a13c3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(f\"CUDA available: {use_cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e4f664d-c209-4b34-a5f9-e3c14da39f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from simpletransformers.seq2seq import Seq2SeqModel, Seq2SeqArgs\n",
    "import pandas as pd\n",
    "import logging\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd3a24b",
   "metadata": {},
   "source": [
    "define loading function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45e6bc66-58a4-4df5-bb94-4b2a60df6f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_data(file_path, input_text_column, target_text_column, label_column, keep_label=1):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep='\\t', on_bad_lines='skip')\n",
    "    except pd.errors.ParserError:\n",
    "        df = pd.read_csv(file_path, sep=',', on_bad_lines='skip')\n",
    "    df = df[df[label_column] == keep_label]\n",
    "    df = df.rename(columns={input_text_column: 'input_text', target_text_column: 'target_text'})\n",
    "    df = df[['input_text', 'target_text']]\n",
    "    return df\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cba84b9c-3c6f-45b0-8f66-4f1dc20d05e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#cleaning function \n",
    "def clean_unnecessary_spaces(out_string):\n",
    "    if not isinstance(out_string, str):\n",
    "        # Convert non-string types to string without issuing a warning\n",
    "        out_string = str(out_string)\n",
    "\n",
    "    # Clean unnecessary spaces in the string\n",
    "    out_string = (\n",
    "        out_string.replace(\".\",\".\")\n",
    "        .replace(\" ?\",\"?\")\n",
    "        .replace(\" !\",\"!\")\n",
    "        .replace(\" ,\",\",\")\n",
    "        .replace(\" ' \",\"'\")\n",
    "        .replace(\" n't\",\"n't\")\n",
    "        .replace(\" 's\",\"'s\")\n",
    "        .replace(\" 'm\",\"'m\")\n",
    "        .replace(\" 're\",\"'re\")\n",
    "        .replace(\" 've\",\"'ve\")\n",
    "        .replace(\" 'll\",\"'ll\")\n",
    "    )\n",
    "    return out_string\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e4437ab-7ecd-4b23-a92e-7d0d69155213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              input_text  \\\n",
      "1      The NBA season of 1975 -- 76 was the 30th seas...   \n",
      "3      When comparable rates of flow can be maintaine...   \n",
      "4      It is the seat of Zerendi District in Akmola R...   \n",
      "5      William Henry Henry Harman was born on 17 Febr...   \n",
      "7      With a discrete amount of probabilities Formul...   \n",
      "...                                                  ...   \n",
      "49384  The Romanesque language , Galician ( Galego ) ...   \n",
      "49390  Note that k is a vector consisting of three in...   \n",
      "49393  Tim Henman won in the final 6 -- 2 , 7 -- 6 , ...   \n",
      "49395  He was considered an active member of the coun...   \n",
      "49397  She was in Cork on June 24 and arrived on 8 Ju...   \n",
      "\n",
      "                                             target_text      prefix  \n",
      "1      The 1975 -- 76 season of the National Basketba...  paraphrase  \n",
      "3      The results are high when comparable flow rate...  paraphrase  \n",
      "4      It is the seat of the district of Zerendi in A...  paraphrase  \n",
      "5      William Henry Harman was born in Waynesboro , ...  paraphrase  \n",
      "7      Given a discrete set of probabilities formula ...  paraphrase  \n",
      "...                                                  ...         ...  \n",
      "49384  The Romance language currently spoken in Galic...  paraphrase  \n",
      "49390  It is necessary to note that k is a vector con...  paraphrase  \n",
      "49393  Tim Tim Henman won 6 -- 2 , 7 -- 6 against Yev...  paraphrase  \n",
      "49395  He was considered an active member of the Coun...  paraphrase  \n",
      "49397  She was at Cork on 24 June , and arrived in th...  paraphrase  \n",
      "\n",
      "[21829 rows x 3 columns]\n",
      "---------\n",
      "                                             input_text  \\\n",
      "1     They were there to enjoy us and they were ther...   \n",
      "2     After the end of the war in June 1902 , Higgin...   \n",
      "3     From the merger of the Four Rivers Council and...   \n",
      "4     The group toured extensively and became famous...   \n",
      "5     Kathy and her husband Pete Beale ( Peter Dean ...   \n",
      "...                                                 ...   \n",
      "7993  In Advent , the traditional `` Tauberbischofsh...   \n",
      "7994  In 2002 , the song was released by British pro...   \n",
      "7995  Tommy Connolly , who plays Rory Jennings , pla...   \n",
      "7996  Monroe Meadows , in Yosemite valley near Brida...   \n",
      "7998  In 2014 the site launched iOS and Android appl...   \n",
      "\n",
      "                                            target_text      prefix  \n",
      "1     They were there for us to enjoy and they were ...  paraphrase  \n",
      "2     In August , after the end of the war in June 1...  paraphrase  \n",
      "3     Shawnee Trails Council was formed from the mer...  paraphrase  \n",
      "4     The group toured extensively and was famous in...  paraphrase  \n",
      "5     Kathy and her husband Peter Dean ( Pete Beale ...  paraphrase  \n",
      "...                                                 ...         ...  \n",
      "7993  During Advent , the traditional `` Tauberbisch...  paraphrase  \n",
      "7994  In 2002 , the song was published by the Britis...  paraphrase  \n",
      "7995  Tommy Connolly , who plays Rory Jennings , pla...  paraphrase  \n",
      "7996  Monroe Meadows , in Yosemite Valley near Brida...  paraphrase  \n",
      "7998  In 2014 launched the site iOS and Android - ap...  paraphrase  \n",
      "\n",
      "[3539 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#Google Data\n",
    "train_df = pd.read_csv(\"train (1).tsv\", sep=\"\\t\").astype(str)\n",
    "eval_df = pd.read_csv(\"dev (1).tsv\", sep=\"\\t\").astype(str)\n",
    "\n",
    "train_df = train_df.loc[train_df[\"label\"] == \"1\"]\n",
    "eval_df = eval_df.loc[eval_df[\"label\"] == \"1\"]\n",
    "\n",
    "train_df = train_df.rename(\n",
    "    columns={\"sentence1\": \"input_text\", \"sentence2\": \"target_text\"}\n",
    ")\n",
    "eval_df = eval_df.rename(\n",
    "    columns={\"sentence1\": \"input_text\", \"sentence2\": \"target_text\"}\n",
    ")\n",
    "\n",
    "train_df = train_df[[\"input_text\", \"target_text\"]]\n",
    "eval_df = eval_df[[\"input_text\", \"target_text\"]]\n",
    "\n",
    "train_df[\"prefix\"] = \"paraphrase\"\n",
    "eval_df[\"prefix\"] = \"paraphrase\"\n",
    "\n",
    "print(train_df)\n",
    "print(\"---------\")\n",
    "print(eval_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "142e2cb8-e023-4922-a9c1-89faad0af49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#MSRP Data\n",
    "train_df = pd.concat(\n",
    "    [\n",
    "        train_df,\n",
    "        load_data(\"msr_paraphrase_train (1).txt\", \"#1 String\", \"#2 String\", \"Quality\"),\n",
    "    ]\n",
    ")\n",
    "eval_df = pd.concat(\n",
    "    [\n",
    "        eval_df,\n",
    "        load_data(\"msr_paraphrase_test (1).txt\", \"#1 String\", \"#2 String\", \"Quality\"),\n",
    "    ]\n",
    ")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f8b12ee-1e4e-4e3f-894e-f8eb5c8af55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#Quora Data\n",
    "# The Quora Dataset is not separated into train/test, so we do it manually the first time.\n",
    "df = load_data(\n",
    "    \"quora_duplicate_questions (1).tsv\", \"question1\", \"question2\", \"is_duplicate\"\n",
    ")\n",
    "q_train, q_test = train_test_split(df)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f90d3b3e-1865-49f1-9011-54f57b2d4000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1d2dd7fae934bcaae44099790034693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96270 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f18eaa0daad463584c9841c6985c778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4b98aae748a48a7afff6f023b51bd5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 1:   0%|          | 0/6017 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 49\u001b[0m\n\u001b[0;32m     42\u001b[0m model_args\u001b[38;5;241m.\u001b[39mwandb_project \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     43\u001b[0m model \u001b[38;5;241m=\u001b[39m Seq2SeqModel(\n\u001b[0;32m     44\u001b[0m     encoder_decoder_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbart\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     45\u001b[0m     encoder_decoder_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mbest_model\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     46\u001b[0m     args\u001b[38;5;241m=\u001b[39mmodel_args,\n\u001b[0;32m     47\u001b[0m     use_cuda\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     48\u001b[0m )\n\u001b[1;32m---> 49\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain_model(train_df, eval_data\u001b[38;5;241m=\u001b[39meval_df, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     50\u001b[0m to_predict \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     51\u001b[0m     prefix \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(input_text)\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m prefix, input_text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(eval_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefix\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist(), eval_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m     53\u001b[0m ]\n\u001b[0;32m     54\u001b[0m truth \u001b[38;5;241m=\u001b[39m eval_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[1;32mc:\\Users\\athar\\anaconda3\\Lib\\site-packages\\simpletransformers\\seq2seq\\seq2seq_model.py:310\u001b[0m, in \u001b[0;36mSeq2SeqModel.train_model\u001b[1;34m(self, train_data, output_dir, show_running_loss, args, eval_data, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    306\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_and_cache_examples(train_data, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[0;32m    308\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 310\u001b[0m global_step, training_details \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain(\n\u001b[0;32m    311\u001b[0m     train_dataset,\n\u001b[0;32m    312\u001b[0m     output_dir,\n\u001b[0;32m    313\u001b[0m     show_running_loss\u001b[38;5;241m=\u001b[39mshow_running_loss,\n\u001b[0;32m    314\u001b[0m     eval_data\u001b[38;5;241m=\u001b[39meval_data,\n\u001b[0;32m    315\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    317\u001b[0m )\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39moutput_dir, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m    321\u001b[0m \u001b[38;5;66;03m# model_to_save = self.model.module if hasattr(self.model, \"module\") else self.model\u001b[39;00m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;66;03m# model_to_save.save_pretrained(output_dir)\u001b[39;00m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;66;03m# self.encoder_tokenizer.save_pretrained(output_dir)\u001b[39;00m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;66;03m# self.decoder_tokenizer.save_pretrained(output_dir)\u001b[39;00m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;66;03m# torch.save(self.args, os.path.join(output_dir, \"training_args.bin\"))\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\athar\\anaconda3\\Lib\\site-packages\\simpletransformers\\seq2seq\\seq2seq_model.py:595\u001b[0m, in \u001b[0;36mSeq2SeqModel.train\u001b[1;34m(self, train_dataset, output_dir, show_running_loss, eval_data, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    593\u001b[0m     scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 595\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    596\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Update learning rate schedule\u001b[39;00m\n\u001b[0;32m    597\u001b[0m model\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\athar\\anaconda3\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:130\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m opt \u001b[38;5;241m=\u001b[39m opt_ref()\n\u001b[0;32m    129\u001b[0m opt\u001b[38;5;241m.\u001b[39m_opt_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(opt, opt\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\athar\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:484\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    479\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    480\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    481\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    482\u001b[0m             )\n\u001b[1;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    487\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\athar\\anaconda3\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\athar\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:637\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    635\u001b[0m     state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_avg\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(p)\n\u001b[0;32m    636\u001b[0m     \u001b[38;5;66;03m# Exponential moving average of squared gradient values\u001b[39;00m\n\u001b[1;32m--> 637\u001b[0m     state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_avg_sq\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(p)\n\u001b[0;32m    639\u001b[0m exp_avg, exp_avg_sq \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_avg\u001b[39m\u001b[38;5;124m\"\u001b[39m], state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_avg_sq\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    640\u001b[0m beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "#parabank dataset 5m\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from simpletransformers.seq2seq import Seq2SeqModel, Seq2SeqArgs\n",
    "from sklearn.model_selection import train_test_split\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "paradata = pd.read_csv(\"parabank_5m (1).tsv\", sep='\\t', header=None, on_bad_lines='skip')\n",
    "paradata = paradata[0:100000]\n",
    "paradata['prefix'] = 'paraphrase'\n",
    "paradata.rename(columns={0: 'input_text', 1: 'target_text'}, inplace=True)\n",
    "para_train, para_test = train_test_split(paradata)\n",
    "train_df = pd.concat([train_df, q_train, para_train])\n",
    "eval_df = pd.concat([eval_df, q_test, para_test])\n",
    "train_df = train_df[[\"prefix\", \"input_text\", \"target_text\"]].dropna()\n",
    "eval_df = eval_df[[\"prefix\", \"input_text\", \"target_text\"]].dropna()\n",
    "train_df[\"input_text\"] = train_df[\"input_text\"].apply(clean_unnecessary_spaces)\n",
    "train_df[\"target_text\"] = train_df[\"target_text\"].apply(clean_unnecessary_spaces)\n",
    "eval_df[\"input_text\"] = eval_df[\"input_text\"].apply(clean_unnecessary_spaces)\n",
    "eval_df[\"target_text\"] = eval_df[\"target_text\"].apply(clean_unnecessary_spaces)\n",
    "model_args = Seq2SeqArgs()\n",
    "model_args.do_sample = True\n",
    "model_args.eval_batch_size = 32\n",
    "model_args.evaluate_during_training = True\n",
    "model_args.evaluate_during_training_steps = 1000\n",
    "model_args.evaluate_during_training_verbose = True\n",
    "model_args.fp16 = False\n",
    "model_args.learning_rate = 5e-5\n",
    "model_args.max_length = 64\n",
    "model_args.max_seq_length = 64\n",
    "model_args.num_beams = None\n",
    "model_args.num_return_sequences = 3\n",
    "model_args.num_train_epochs = 1\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.reprocess_input_data = True\n",
    "model_args.save_eval_checkpoints = True\n",
    "model_args.save_steps = 2000\n",
    "model_args.evaluate_during_training_verbose = True\n",
    "model_args.top_k = 50\n",
    "model_args.top_p = 0.95\n",
    "model_args.train_batch_size = 16\n",
    "model_args.use_multiprocessing = False\n",
    "model_args.wandb_project = None\n",
    "model = Seq2SeqModel(\n",
    "    encoder_decoder_type=\"bart\",\n",
    "    encoder_decoder_name=r\"outputs\\best_model\",\n",
    "    args=model_args,\n",
    "    use_cuda=False\n",
    ")\n",
    "model.train_model(train_df, eval_data=eval_df, verbose=True)\n",
    "to_predict = [\n",
    "    prefix + \": \" + str(input_text)\n",
    "    for prefix, input_text in zip(eval_df[\"prefix\"].tolist(), eval_df[\"input_text\"].tolist())\n",
    "]\n",
    "truth = eval_df[\"target_text\"].tolist()\n",
    "preds = model.predict(to_predict)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2459a908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a8602e5f5124a18ae2b3509bafd011d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\athar\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:615: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\athar\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:634: UserWarning: `num_beams` is set to 1. However, `length_penalty` is set to `2.0` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `length_penalty`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [['paraphrase: he is a good player', 'paraphrase: Do it he is a good player', 'paraphrase: he is a good player', 'paraphrase: he is a good player home defence player I am a good...', 'paraphrase: he is a good player', 'paraphrase: he is a good operator and a good player']]\n",
      "Paraphrase 1: par he is a good player\n",
      "Paraphrase 2: par Do it he is a good player\n",
      "Paraphrase 3: par he is a good player\n",
      "Paraphrase 4: par he is a good player home defence player I am a good...\n",
      "Paraphrase 5: par he is a good player\n",
      "Paraphrase 6: par he is a good operator and a good player\n"
     ]
    }
   ],
   "source": [
    "from simpletransformers.seq2seq import Seq2SeqModel, Seq2SeqArgs\n",
    "\n",
    "# Load the fine-tuned BART model for paraphrasing (checkpoint-6020-epoch-1)\n",
    "model_args = Seq2SeqArgs()\n",
    "model_args.do_sample = True\n",
    "model_args.eval_batch_size = 32\n",
    "model_args.max_length = 64\n",
    "model_args.num_return_sequences = 1  # Default number of outputs for paraphrasing\n",
    "\n",
    "paraphrase_model = Seq2SeqModel(\n",
    "    encoder_decoder_type=\"bart\",\n",
    "    encoder_decoder_name=r\"outputs\\best_model\",  # Updated path to fine-tuned model\n",
    "    args=model_args,\n",
    "    use_cuda=False\n",
    ")\n",
    "\n",
    "# Function for paraphrasing\n",
    "def paraphrase_text(input_text, num_return_sequences=1):\n",
    "    model_args.num_return_sequences = num_return_sequences\n",
    "    to_predict = [f\"paraphrase: {input_text.strip()}\"]\n",
    "    predictions = paraphrase_model.predict(to_predict)\n",
    "    \n",
    "    # Print predictions to debug\n",
    "    print(\"Predictions:\", predictions)\n",
    "    \n",
    "    # Flatten predictions if it's a list of lists\n",
    "    if isinstance(predictions[0], list):\n",
    "        flattened_predictions = [p for sublist in predictions for p in sublist]  # Flattening the list of lists\n",
    "        return clean_paraphrase_output(flattened_predictions)  # Clean the flattened predictions\n",
    "    \n",
    "    return clean_paraphrase_output(predictions) if predictions else [\"No output generated.\"]\n",
    "\n",
    "# Function to clean paraphrase output\n",
    "# Function to clean paraphrase output\n",
    "def clean_paraphrase_output(predictions):\n",
    "    cleaned_output = []\n",
    "    for p in predictions:\n",
    "        if isinstance(p, str):\n",
    "            # Remove common prefixes and extra spaces\n",
    "            cleaned_p = (\n",
    "                p.replace(\"aphrase:\", \"\")\n",
    "                .replace(\"aphent:\", \"\")\n",
    "                .replace(\"aphdomenas:\", \"\")\n",
    "                .replace(\"aphdomenas :\", \"\")\n",
    "                .replace(\"aphrase :\", \"\")\n",
    "                .replace(\"aphrase :\", \"\")\n",
    "                .replace(\" aphrase:\", \"\")\n",
    "                .replace(\" aphrase :\", \"\")\n",
    "                .replace(\"  aphrase :\", \"\")\n",
    "                .replace(\"  aphrase:\", \"\")\n",
    "                .replace(\" aphrase : \", \"\")\n",
    "                .strip()\n",
    "            )\n",
    "            cleaned_output.append(cleaned_p)\n",
    "    return cleaned_output\n",
    "\n",
    "\n",
    "# Example input\n",
    "input_text = \"he is a good player \"\n",
    "num_paraphrases = 6  \n",
    "\n",
    "# Get paraphrased results\n",
    "result = paraphrase_text(input_text, num_return_sequences=num_paraphrases)\n",
    "\n",
    "# Display the results\n",
    "for idx, paraphrase in enumerate(result):\n",
    "    print(f\"Paraphrase {idx + 1}: {paraphrase}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15e96169-9fa1-49af-b307-4b89633923f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       prefix                                         input_text  \\\n",
      "1  paraphrase  The NBA season of 1975 -- 76 was the 30th seas...   \n",
      "3  paraphrase  When comparable rates of flow can be maintaine...   \n",
      "4  paraphrase  It is the seat of Zerendi District in Akmola R...   \n",
      "5  paraphrase  William Henry Henry Harman was born on 17 Febr...   \n",
      "7  paraphrase  With a discrete amount of probabilities Formul...   \n",
      "\n",
      "                                         target_text  \n",
      "1  The 1975 -- 76 season of the National Basketba...  \n",
      "3  The results are high when comparable flow rate...  \n",
      "4  It is the seat of the district of Zerendi in A...  \n",
      "5  William Henry Harman was born in Waynesboro, V...  \n",
      "7  Given a discrete set of probabilities formula ...  \n",
      "Index(['prefix', 'input_text', 'target_text'], dtype='object')\n",
      "       prefix                                         input_text  \\\n",
      "1  paraphrase  They were there to enjoy us and they were ther...   \n",
      "2  paraphrase  After the end of the war in June 1902, Higgins...   \n",
      "3  paraphrase  From the merger of the Four Rivers Council and...   \n",
      "4  paraphrase  The group toured extensively and became famous...   \n",
      "5  paraphrase  Kathy and her husband Pete Beale ( Peter Dean ...   \n",
      "\n",
      "                                         target_text  \n",
      "1  They were there for us to enjoy and they were ...  \n",
      "2  In August, after the end of the war in June 19...  \n",
      "3  Shawnee Trails Council was formed from the mer...  \n",
      "4  The group toured extensively and was famous in...  \n",
      "5  Kathy and her husband Peter Dean ( Pete Beale ...  \n",
      "Index(['prefix', 'input_text', 'target_text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train_df.head())\n",
    "print(train_df.columns)\n",
    "print(eval_df.head())\n",
    "print(eval_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f82e44df-625c-4bd8-822a-80aa3f479022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefix         object\n",
      "input_text     object\n",
      "target_text    object\n",
      "dtype: object\n",
      "prefix         object\n",
      "input_text     object\n",
      "target_text    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train_df.dtypes)\n",
    "print(eval_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e507209f-b646-4406-8981-c8af12695f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Missing Values:\n",
      " prefix         0\n",
      "input_text     0\n",
      "target_text    0\n",
      "dtype: int64\n",
      "Evaluation Data Missing Values:\n",
      " prefix         0\n",
      "input_text     0\n",
      "target_text    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Data Missing Values:\\n\", train_df.isnull().sum())\n",
    "print(\"Evaluation Data Missing Values:\\n\", eval_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c222a1f-8a9e-41f8-8ca9-42844595d100",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\athar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 58\u001b[0m\n\u001b[0;32m     55\u001b[0m tsv_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest (1).tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Relative path to the file\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Run the evaluation\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m evaluate_model(tsv_file)\n",
      "Cell \u001b[1;32mIn[14], line 37\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(tsv_file)\u001b[0m\n\u001b[0;32m     34\u001b[0m true_paraphrase \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence2\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Expected paraphrase (target)\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Generate paraphrase using the model\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m generated_paraphrase \u001b[38;5;241m=\u001b[39m generate_paraphrase(input_sentence)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Tokenize the reference and hypothesis sentences\u001b[39;00m\n\u001b[0;32m     40\u001b[0m reference_tokens \u001b[38;5;241m=\u001b[39m [nltk\u001b[38;5;241m.\u001b[39mword_tokenize(true_paraphrase)]\n",
      "Cell \u001b[1;32mIn[14], line 21\u001b[0m, in \u001b[0;36mgenerate_paraphrase\u001b[1;34m(input_sentence)\u001b[0m\n\u001b[0;32m     18\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Generate paraphrase\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(input_ids\u001b[38;5;241m=\u001b[39minput_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mdecode(output[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\athar\\anaconda3\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\athar\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:2063\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   2055\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2056\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2057\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   2058\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2059\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2060\u001b[0m     )\n\u001b[0;32m   2062\u001b[0m     \u001b[38;5;66;03m# 14. run beam sample\u001b[39;00m\n\u001b[1;32m-> 2063\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_beam_search(\n\u001b[0;32m   2064\u001b[0m         input_ids,\n\u001b[0;32m   2065\u001b[0m         beam_scorer,\n\u001b[0;32m   2066\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2067\u001b[0m         logits_warper\u001b[38;5;241m=\u001b[39mprepared_logits_warper,\n\u001b[0;32m   2068\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2069\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2070\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   2071\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2072\u001b[0m     )\n\u001b[0;32m   2074\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGROUP_BEAM_SEARCH:\n\u001b[0;32m   2075\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   2076\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   2077\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   2078\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2084\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   2085\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\athar\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:3251\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[1;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, logits_warper, **model_kwargs)\u001b[0m\n\u001b[0;32m   3246\u001b[0m next_token_logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\u001b[38;5;241m.\u001b[39mclone()\n\u001b[0;32m   3247\u001b[0m next_token_scores \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mlog_softmax(\n\u001b[0;32m   3248\u001b[0m     next_token_logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   3249\u001b[0m )  \u001b[38;5;66;03m# (batch_size * num_beams, vocab_size)\u001b[39;00m\n\u001b[1;32m-> 3251\u001b[0m next_token_scores_processed \u001b[38;5;241m=\u001b[39m logits_processor(input_ids, next_token_scores)\n\u001b[0;32m   3252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_sample:\n\u001b[0;32m   3253\u001b[0m     next_token_scores_processed \u001b[38;5;241m=\u001b[39m logits_warper(input_ids, next_token_scores_processed)\n",
      "File \u001b[1;32mc:\\Users\\athar\\anaconda3\\Lib\\site-packages\\transformers\\generation\\logits_process.py:98\u001b[0m, in \u001b[0;36mLogitsProcessorList.__call__\u001b[1;34m(self, input_ids, scores, **kwargs)\u001b[0m\n\u001b[0;32m     96\u001b[0m         scores \u001b[38;5;241m=\u001b[39m processor(input_ids, scores, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 98\u001b[0m         scores \u001b[38;5;241m=\u001b[39m processor(input_ids, scores)\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "File \u001b[1;32mc:\\Users\\athar\\anaconda3\\Lib\\site-packages\\transformers\\generation\\logits_process.py:967\u001b[0m, in \u001b[0;36mNoRepeatNGramLogitsProcessor.__call__\u001b[1;34m(self, input_ids, scores)\u001b[0m\n\u001b[0;32m    965\u001b[0m num_batch_hypotheses \u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    966\u001b[0m cur_len \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 967\u001b[0m scores_processed \u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39mclone()\n\u001b[0;32m    968\u001b[0m banned_batch_tokens \u001b[38;5;241m=\u001b[39m _calc_banned_ngram_tokens(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mngram_size, input_ids, num_batch_hypotheses, cur_len)\n\u001b[0;32m    969\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, banned_tokens \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(banned_batch_tokens):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import nltk\n",
    "nltk.download('punkt')  # Ensure that necessary NLTK data is downloaded\n",
    "\n",
    "# Load your trained model and tokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained(r\"outputs\\best_model\")\n",
    "model = BartForConditionalGeneration.from_pretrained(r\"outputs\\best_model\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Function to generate paraphrases using the custom model\n",
    "def generate_paraphrase(input_sentence):\n",
    "    encoding = tokenizer.encode_plus(input_sentence, return_tensors=\"pt\", padding=True)\n",
    "    input_ids = encoding[\"input_ids\"].to(device)\n",
    "    attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "    # Generate paraphrase\n",
    "    output = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=128)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Function to evaluate the model accuracy\n",
    "def evaluate_model(tsv_file):\n",
    "    # Load the TSV file into a pandas DataFrame\n",
    "    eval_df = pd.read_csv(tsv_file, sep='\\t')\n",
    "    \n",
    "    references = []  # Store the true paraphrases (sentence2)\n",
    "    hypotheses = []  # Store the generated paraphrases (model output)\n",
    "    \n",
    "    for _, row in eval_df.iterrows():\n",
    "        input_sentence = row['sentence1']  # Original sentence (input)\n",
    "        true_paraphrase = row['sentence2']  # Expected paraphrase (target)\n",
    "        \n",
    "        # Generate paraphrase using the model\n",
    "        generated_paraphrase = generate_paraphrase(input_sentence)\n",
    "        \n",
    "        # Tokenize the reference and hypothesis sentences\n",
    "        reference_tokens = [nltk.word_tokenize(true_paraphrase)]\n",
    "        hypothesis_tokens = nltk.word_tokenize(generated_paraphrase)\n",
    "        \n",
    "        references.append(reference_tokens)\n",
    "        hypotheses.append(hypothesis_tokens)\n",
    "\n",
    "    # Calculate BLEU score for each sentence\n",
    "    bleu_scores = [sentence_bleu(ref, hyp) for ref, hyp in zip(references, hypotheses)]\n",
    "    \n",
    "    # Calculate the average BLEU score across all sentences\n",
    "    avg_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
    "    \n",
    "    print(f\"Average BLEU Score: {avg_bleu_score:.4f}\")\n",
    "\n",
    "# Path to your TSV file\n",
    "tsv_file = \"test (1).tsv\"  # Relative path to the file\n",
    "\n",
    "# Run the evaluation\n",
    "evaluate_model(tsv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861734c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install simpletransformers==0.60.9\n",
    "# import torch\n",
    "# use_cuda = torch.cuda.is_available()\n",
    "# print(f\"CUDA available: {use_cuda}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from simpletransformers.seq2seq import Seq2SeqModel, Seq2SeqArgs\n",
    "# import pandas as pd\n",
    "# import logging\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import os\n",
    "# from datetime import datetime\n",
    "# import warnings\n",
    "# print(\"done\")\n",
    "# def load_data(file_path, input_text_column, target_text_column, label_column, keep_label=1):\n",
    "#     try:\n",
    "#         # Load data, skipping bad lines\n",
    "#         df = pd.read_csv(file_path, sep='\\t', on_bad_lines='skip')\n",
    "#     except pd.errors.ParserError:\n",
    "#         # If the file uses a different delimiter, try with comma\n",
    "#         df = pd.read_csv(file_path, sep=',', on_bad_lines='skip')\n",
    "#     df = df[df[label_column] == keep_label]\n",
    "#     df = df.rename(columns={input_text_column: 'input_text', target_text_column: 'target_text'})\n",
    "#     df = df[['input_text', 'target_text']]\n",
    "\n",
    "#     return df\n",
    "# print(\"done\")\n",
    "# import warnings\n",
    "\n",
    "# def clean_unnecessary_spaces(out_string):\n",
    "#     if not isinstance(out_string, str):\n",
    "#         # Convert non-string types to string without issuing a warning\n",
    "#         out_string = str(out_string)\n",
    "\n",
    "#     # Clean unnecessary spaces in the string\n",
    "#     out_string = (\n",
    "#         out_string.replace(\".\",\".\")\n",
    "#         .replace(\" ?\",\"?\")\n",
    "#         .replace(\" !\",\"!\")\n",
    "#         .replace(\" ,\",\",\")\n",
    "#         .replace(\" ' \",\"'\")\n",
    "#         .replace(\" n't\",\"n't\")\n",
    "#         .replace(\" 's\",\"'s\")\n",
    "#         .replace(\" 'm\",\"'m\")\n",
    "#         .replace(\" 're\",\"'re\")\n",
    "#         .replace(\" 've\",\"'ve\")\n",
    "#         .replace(\" 'll\",\"'ll\")\n",
    "#     )\n",
    "#     return out_string\n",
    "# print(\"done\")\n",
    "# #Google Data\n",
    "# train_df = pd.read_csv(\"train (1).tsv\", sep=\"\\t\").astype(str)\n",
    "# eval_df = pd.read_csv(\"dev (1).tsv\", sep=\"\\t\").astype(str)\n",
    "\n",
    "# train_df = train_df.loc[train_df[\"label\"] == \"1\"]\n",
    "# eval_df = eval_df.loc[eval_df[\"label\"] == \"1\"]\n",
    "\n",
    "# train_df = train_df.rename(\n",
    "#     columns={\"sentence1\": \"input_text\", \"sentence2\": \"target_text\"}\n",
    "# )\n",
    "# eval_df = eval_df.rename(\n",
    "#     columns={\"sentence1\": \"input_text\", \"sentence2\": \"target_text\"}\n",
    "# )\n",
    "\n",
    "# train_df = train_df[[\"input_text\", \"target_text\"]]\n",
    "# eval_df = eval_df[[\"input_text\", \"target_text\"]]\n",
    "\n",
    "# train_df[\"prefix\"] = \"paraphrase\"\n",
    "# eval_df[\"prefix\"] = \"paraphrase\"\n",
    "\n",
    "# print(train_df)\n",
    "# print(\"---------\")\n",
    "# print(eval_df)\n",
    "\n",
    "# #MSRP Data\n",
    "# train_df = pd.concat(\n",
    "#     [\n",
    "#         train_df,\n",
    "#         load_data(\"msr_paraphrase_train (1).txt\", \"#1 String\", \"#2 String\", \"Quality\"),\n",
    "#     ]\n",
    "# )\n",
    "# eval_df = pd.concat(\n",
    "#     [\n",
    "#         eval_df,\n",
    "#         load_data(\"msr_paraphrase_test (1).txt\", \"#1 String\", \"#2 String\", \"Quality\"),\n",
    "#     ]\n",
    "# )\n",
    "# print(\"done\")\n",
    "# #Quora Data\n",
    "# # The Quora Dataset is not separated into train/test, so we do it manually the first time.\n",
    "# df = load_data(\n",
    "#     \"quora_duplicate_questions (1).tsv\", \"question1\", \"question2\", \"is_duplicate\"\n",
    "# )\n",
    "# q_train, q_test = train_test_split(df)\n",
    "# print(\"done\")\n",
    "# import warnings\n",
    "# import pandas as pd\n",
    "# from simpletransformers.seq2seq import Seq2SeqModel, Seq2SeqArgs\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "# paradata = pd.read_csv(\"parabank_5m (1).tsv\", sep='\\t', header=None, on_bad_lines='skip')\n",
    "# paradata = paradata[0:100000]\n",
    "# paradata['prefix'] = 'paraphrase'\n",
    "# paradata.rename(columns={0: 'input_text', 1: 'target_text'}, inplace=True)\n",
    "# para_train, para_test = train_test_split(paradata)\n",
    "# train_df = pd.concat([train_df, q_train, para_train])\n",
    "# eval_df = pd.concat([eval_df, q_test, para_test])\n",
    "# train_df = train_df[[\"prefix\", \"input_text\", \"target_text\"]].dropna()\n",
    "# eval_df = eval_df[[\"prefix\", \"input_text\", \"target_text\"]].dropna()\n",
    "# train_df[\"input_text\"] = train_df[\"input_text\"].apply(clean_unnecessary_spaces)\n",
    "# train_df[\"target_text\"] = train_df[\"target_text\"].apply(clean_unnecessary_spaces)\n",
    "# eval_df[\"input_text\"] = eval_df[\"input_text\"].apply(clean_unnecessary_spaces)\n",
    "# eval_df[\"target_text\"] = eval_df[\"target_text\"].apply(clean_unnecessary_spaces)\n",
    "# model_args = Seq2SeqArgs()\n",
    "# model_args.do_sample = True\n",
    "# model_args.eval_batch_size = 32\n",
    "# model_args.evaluate_during_training = True\n",
    "# model_args.evaluate_during_training_steps = 1000\n",
    "# model_args.evaluate_during_training_verbose = True\n",
    "# model_args.fp16 = False\n",
    "# model_args.learning_rate = 5e-5\n",
    "# model_args.max_length = 64\n",
    "# model_args.max_seq_length = 64\n",
    "# model_args.num_beams = None\n",
    "# model_args.num_return_sequences = 3\n",
    "# model_args.num_train_epochs = 1\n",
    "# model_args.overwrite_output_dir = True\n",
    "# model_args.reprocess_input_data = True\n",
    "# model_args.save_eval_checkpoints = True\n",
    "# model_args.save_steps = 2000\n",
    "# model_args.evaluate_during_training_verbose = True\n",
    "# model_args.top_k = 50\n",
    "# model_args.top_p = 0.95\n",
    "# model_args.train_batch_size = 16\n",
    "# model_args.use_multiprocessing = False\n",
    "# model_args.wandb_project = None\n",
    "# model = Seq2SeqModel(\n",
    "#     encoder_decoder_type=\"bart\",\n",
    "#     encoder_decoder_name=r\"outputs\\checkpoint-6019-epoch-1\",\n",
    "#     args=model_args,\n",
    "#     use_cuda=False\n",
    "# )\n",
    "# model.train_model(train_df, eval_data=eval_df, verbose=True)\n",
    "# to_predict = [\n",
    "#     prefix + \": \" + str(input_text)\n",
    "#     for prefix, input_text in zip(eval_df[\"prefix\"].tolist(), eval_df[\"input_text\"].tolist())\n",
    "# ]\n",
    "# truth = eval_df[\"target_text\"].tolist()\n",
    "# preds = model.predict(to_predict)\n",
    "# print(\"done\")\n",
    "# import warnings\n",
    "# import pandas as pd\n",
    "# from simpletransformers.seq2seq import Seq2SeqModel, Seq2SeqArgs\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# paradata = pd.read_csv(\"parabank_5m (1).tsv\", sep='\\t', header=None, on_bad_lines='skip')\n",
    "# paradata = paradata[0:100000]\n",
    "# paradata['prefix'] = 'paraphrase'\n",
    "# paradata.rename(columns={0: 'input_text', 1: 'target_text'}, inplace=True)\n",
    "\n",
    "# para_train, para_test = train_test_split(paradata)\n",
    "# train_df = para_train[[\"prefix\", \"input_text\", \"target_text\"]].dropna()\n",
    "# eval_df = para_test[[\"prefix\", \"input_text\", \"target_text\"]].dropna()\n",
    "\n",
    "# def clean_unnecessary_spaces(text):\n",
    "#     return \" \".join(text.split())\n",
    "\n",
    "# train_df[\"input_text\"] = train_df[\"input_text\"].apply(clean_unnecessary_spaces)\n",
    "# train_df[\"target_text\"] = train_df[\"target_text\"].apply(clean_unnecessary_spaces)\n",
    "# eval_df[\"input_text\"] = eval_df[\"input_text\"].apply(clean_unnecessary_spaces)\n",
    "# eval_df[\"target_text\"] = eval_df[\"target_text\"].apply(clean_unnecessary_spaces)\n",
    "\n",
    "# # Set up model arguments\n",
    "# model_args = Seq2SeqArgs()\n",
    "# model_args.do_sample = True\n",
    "# model_args.eval_batch_size = 32\n",
    "# model_args.max_length = 64\n",
    "# model_args.num_return_sequences = 3\n",
    "# model_args.num_beams = 5  # Set to a value > 1 for better generation\n",
    "# model_args.use_multiprocessing = False\n",
    "\n",
    "# model = Seq2SeqModel(\n",
    "#     encoder_decoder_type=\"bart\",\n",
    "#     encoder_decoder_name=r\"C:\\Users\\athar\\Downloads\\para_project\\outputs\\checkpoint-6019-epoch-1\",\n",
    "#     args=model_args,\n",
    "#     use_cuda=False\n",
    "# )\n",
    "\n",
    "# input_sentences = [\n",
    "#     \"Chess is a board game for two players. It is sometimes called international chess or Western chess to distinguish it from related games such as xiangqi (Chinese chess) and shogi (Japanese chess)\",\n",
    "# ]\n",
    "\n",
    "# to_predict = [\"paraphrase: \" + sentence for sentence in input_sentences]\n",
    "\n",
    "# predictions = model.predict(to_predict)\n",
    "# for input_sentence, pred in zip(input_sentences, predictions):\n",
    "#     print(f\"Input: {input_sentence}\\nParaphrased:\")\n",
    "#     for p in pred:\n",
    "#         print(f\"  - {p.replace('paraphrase: ', '').strip()}\")\n",
    "#     print()  \n",
    "\n",
    "# def generate_paraphrase(input_sentence):\n",
    "#     to_predict = [f\"paraphrase: {input_sentence}\"]\n",
    "#     paraphrase = model.predict(to_predict)\n",
    "#     return paraphrase[0] if paraphrase else None\n",
    "# input_sentence = \"The chocolate cake was delicious and very enjoyable\"\n",
    "# paraphrased_sentence = generate_paraphrase(input_sentence)\n",
    "# print(\"Input Sentence: \", input_sentence)\n",
    "# print( paraphrased_sentence)\n",
    "# print(train_df.head())\n",
    "# print(train_df.columns)\n",
    "# print(eval_df.head())\n",
    "# print(eval_df.columns)\n",
    "# print(\"Training Data Missing Values:\\n\", train_df.isnull().sum())\n",
    "# print(\"Evaluation Data Missing Values:\\n\", eval_df.isnull().sum())\n",
    "# import torchprint(\"Training Data Missing Values:\\n\", train_df.isnull().sum())\n",
    "# print(\"Evaluation Data Missing Values:\\n\", eval_df.isnull().sum())\n",
    "# import pandas as pd\n",
    "# from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "# from nltk.translate.bleu_score import sentence_bleu\n",
    "# import nltk\n",
    "# nltk.download('punkt')  # Ensure that necessary NLTK data is downloaded\n",
    "\n",
    "# # Load your trained model and tokenizer\n",
    "# tokenizer = BartTokenizer.from_pretrained(r\"outputs\\checkpoint-6020-epoch-1\")\n",
    "# model = BartForConditionalGeneration.from_pretrained(r\"outputs\\checkpoint-6020-epoch-1\")\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "# # Function to generate paraphrases using the custom model\n",
    "# def generate_paraphrase(input_sentence):\n",
    "#     encoding = tokenizer.encode_plus(input_sentence, return_tensors=\"pt\", padding=True)\n",
    "#     input_ids = encoding[\"input_ids\"].to(device)\n",
    "#     attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "#     # Generate paraphrase\n",
    "#     output = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=128)\n",
    "#     return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# # Function to evaluate the model accuracy\n",
    "# def evaluate_model(tsv_file):\n",
    "#     # Load the TSV file into a pandas DataFrame\n",
    "#     eval_df = pd.read_csv(tsv_file, sep='\\t')\n",
    "    \n",
    "#     references = []  # Store the true paraphrases (sentence2)\n",
    "#     hypotheses = []  # Store the generated paraphrases (model output)\n",
    "    \n",
    "#     for _, row in eval_df.iterrows():\n",
    "#         input_sentence = row['sentence1']  # Original sentence (input)\n",
    "#         true_paraphrase = row['sentence2']  # Expected paraphrase (target)\n",
    "        \n",
    "#         # Generate paraphrase using the model\n",
    "#         generated_paraphrase = generate_paraphrase(input_sentence)\n",
    "        \n",
    "#         # Tokenize the reference and hypothesis sentences\n",
    "#         reference_tokens = [nltk.word_tokenize(true_paraphrase)]\n",
    "#         hypothesis_tokens = nltk.word_tokenize(generated_paraphrase)\n",
    "        \n",
    "#         references.append(reference_tokens)\n",
    "#         hypotheses.append(hypothesis_tokens)\n",
    "\n",
    "#     # Calculate BLEU score for each sentence\n",
    "#     bleu_scores = [sentence_bleu(ref, hyp) for ref, hyp in zip(references, hypotheses)]\n",
    "    \n",
    "#     # Calculate the average BLEU score across all sentences\n",
    "#     avg_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
    "    \n",
    "#     print(f\"Average BLEU Score: {avg_bleu_score:.4f}\")\n",
    "\n",
    "# # Path to your TSV file\n",
    "# tsv_file = \"test (1).tsv\"  # Relative path to the file\n",
    "\n",
    "# # Run the evaluation\n",
    "# evaluate_model(tsv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47e2895",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdacaf94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0ce9c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b35778",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630bb953",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875ed3d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
